
# OpenAI æœºå™¨äººå¼ºåŒ–å­¦ä¹ ä»å…¥é—¨åˆ°æé«˜ **(è‰ç¨¿)**

> é™¤äº†è¯•å›¾ç›´æ¥å»å»ºç«‹ä¸€ä¸ªå¯ä»¥æ¨¡æ‹Ÿæˆäººå¤§è„‘çš„ç¨‹åºä¹‹å¤–ï¼Œ ä¸ºä»€ä¹ˆä¸è¯•å›¾å»ºç«‹ä¸€ä¸ªå¯ä»¥æ¨¡æ‹Ÿå°å­©å¤§è„‘çš„ç¨‹åºå‘¢?å¦‚æœå®ƒæ¥ å—é€‚å½“çš„æ•™è‚²ï¼Œå°±ä¼šè·å¾—æˆäººçš„å¤§è„‘ã€‚ â€” é˜¿å…°Â·å›¾çµ

## å­¦ä¹ ç›®çš„

- ç†è®ºå’Œä»¿çœŸå®è·µç»“åˆ
- äº†è§£æŒæ¡å¼ºåŒ–å­¦ä¹ åŸºæœ¬åŸç†
- æŒæ¡åˆ©ç”¨ Python è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¿çœŸ

## ä¸€. å¼•è¨€ä»‹ç»

å¼ºåŒ–å­¦ä¹  (Reinforcement learning) æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸç”¨äºåˆ¶å®šå†³ç­–å’Œè¿åŠ¨è‡ªç”±åº¦æ§åˆ¶ã€‚å¼ºåŒ–å­¦ä¹ ä¸»è¦ç ”ç©¶åœ¨å¤æ‚æœªçŸ¥çš„ç¯å¢ƒä¸­ï¼Œæ™ºä½“(agent)å®ç°æŸä¸ªç›®æ ‡ã€‚å¼ºåŒ–å­¦ä¹ æœ€å¼•äººå…¥èƒœçš„ä¸¤ä¸ªç‰¹ç‚¹æ˜¯

- **å¼ºåŒ–å­¦ä¹ éå¸¸é€šç”¨ï¼Œå¯ä»¥ç”¨æ¥è§£å†³éœ€è¦ä½œå‡ºä¸€äº›åˆ—å†³ç­–çš„æ‰€æœ‰é—®é¢˜ï¼š**ä¾‹å¦‚ï¼Œè®­ç»ƒæœºå™¨äººè·‘æ­¥å’Œå¼¹è·³ï¼Œåˆ¶å®šå•†å“ä»·æ ¼å’Œåº“å­˜ç®¡ç†ï¼Œç© Atari æ¸¸æˆå’Œæ£‹ç›˜æ¸¸æˆç­‰ç­‰ã€‚

- **å¼ºåŒ–å­¦ä¹ å·²ç»å¯ä»¥åœ¨è®¸å¤šå¤æ‚çš„ç¯å¢ƒä¸­å–å¾—è¾ƒå¥½çš„å®éªŒç»“æœï¼š**ä¾‹å¦‚ Deep RL çš„ Alpha Goç­‰

[Gym](https://gym.openai.com/docs/) æ˜¯ä¸€ä¸ªç ”ç©¶å’Œå¼€å‘å¼ºåŒ–å­¦ä¹ ç›¸å…³ç®—æ³•çš„ä»¿çœŸå¹³å°ã€‚

- æ— éœ€æ™ºä½“å…ˆéªŒçŸ¥è¯†ï¼›
- å…¼å®¹å¸¸è§çš„æ•°å€¼è¿ç®—åº“å¦‚ TensorFlowã€Theano ç­‰

## äºŒ. å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ

å¼ºåŒ–å­¦ä¹ ä¹Ÿæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å­¦ä¹ çš„ä¸åŒåœ¨ äºï¼Œå¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸éœ€è¦ç»™å‡ºâ€œæ­£ç¡®â€ç­–ç•¥ä½œä¸ºç›‘ç£ä¿¡æ¯ï¼Œåªéœ€è¦ç»™å‡ºç­–ç•¥çš„(å»¶è¿Ÿ)å›æŠ¥ï¼Œå¹¶é€šè¿‡è°ƒæ•´ç­–ç•¥æ¥å–å¾—æœ€å¤§åŒ–çš„æœŸæœ›å›æŠ¥ã€‚

![](https://ws1.sinaimg.cn/large/c3a916a7gy1fuipsc9dnrj20iy05g74p.jpg)

### 2.1 æœ¯è¯­

- æ™ºä½“ (Agent): æ‰§è¡ŒåŠ¨ä½œå¯¹ç¯å¢ƒäº§ç”Ÿå½±å“ï¼Œæ„ŸçŸ¥å¤–æ¥ç¯å¢ƒçŠ¶æ€(state)å’Œåé¦ˆå¥–åŠ±(reward)ï¼Œå¹¶è¿›è¡Œå­¦ä¹ å’Œå†³ç­–ã€‚
- ç¯å¢ƒ (Environment)ï¼š å‡ºäº†æ™ºä½“ä»¥å¤–çš„æ‰€æœ‰äº‹ç‰©ï¼Œæ™ºä½“åŠ¨ä½œå¯ä»¥å½±å“ç¯å¢ƒçŠ¶æ€ï¼Œåé¦ˆæ™ºä½“å¥–åŠ±ã€‚
- çŠ¶æ€ $s$ (State)ï¼šæ˜¯ç¯å¢ƒçš„æè¿°ã€‚
- åŠ¨ä½œ $a$ (Action)ï¼šå¯¹æ™ºä½“è¡Œä¸ºçš„æè¿°ã€‚
- ç­–ç•¥ $\pi$ (Policy)ï¼šæ™ºä½“æ ¹æ®ç¯å¢ƒçš„çŠ¶æ€æ¥å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ$a$çš„å‡½æ•°ã€‚
- å¥–åŠ± $r$ (Reward)ï¼šå½“æ™ºä½“å®ŒæˆåŠ¨ä½œä¹‹åï¼Œç¯å¢ƒä¼šå“åº”çš„ç»™æ™ºä½“ä¸€ä¸ªå¥–åŠ±(æ ‡é‡å€¼)$r$ã€‚
- çŠ¶æ€è½¬ç§»æ¦‚ç‡ ï¼šæ™ºä½“ä»å‰ä¸€ä¸ªçŠ¶æ€å®ŒæˆåŠ¨ä½œåï¼Œç¯å¢ƒåœ¨ä¸‹ä¸ªæ—¶é—´ç‚¹è½¬å˜æˆçŠ¶æ€sçš„æ¦‚ç‡

### 2.2 é©¬å°”ç§‘å¤«è¿‡å†³ç­–è¿‡ç¨‹

#### åŸç†å›¾



![](https://ws1.sinaimg.cn/large/c3a916a7gy1fujcxmkef7j20kp04vaa7.jpg)

#### è§£é‡Š

- æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’çš„è¿‡ç¨‹å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ª**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹**ã€‚é©¬å°”å¯å¤«è¿‡ç¨‹(Markov process)æ˜¯å…·æœ‰é©¬å°”å¯å¤«æ€§çš„éšæœºå˜é‡åºåˆ—$s_0,s_1,...,s_t$ ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€ $s_{t+1}$ åªå–å†³äºå½“å‰æ—¶åˆ»çš„ $s_{t}$ 

$$p\left(s_{t+1} \mid s_t,...,s_0 \right) = p\left(s_{t+1} \mid s_{t}\right)$$

- ç»™å®šç­–ç•¥$\pi\left(a\mid s\right)$ï¼Œè½¨è¿¹

$$\tau = s_0,a_0,s_1,r_1,a_1,...,s_{T-1},s_T,r_T$$

### å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°

- æ€»å›æŠ¥ï¼ŒæŠ˜æ‰£ç‡ $\gamma \in \left[0,1\right]$

$$G_{T}=\sum_{t=0}^{T-1}\gamma ^{t} r_{t+1}$$

- ç›®æ ‡å‡½æ•°

$$J\left( \theta \right) = E\left[G\left(\tau \right)\right]$$

## ä¸‰. OpenAI å¼ºåŒ–å­¦ä¹ ä»¿çœŸç¯å¢ƒ

- A standard Python API for RL environments
- A set of tools to measure agent performance
- An online scoreboard for comparing and benchmarking approaches
- [ https://gym.openai.com/](https://gym.openai.com/)

### 3.1 ç¯å¢ƒå®‰è£…

- pip å®‰è£…  
    ```
    pip3 install gym
    ```
- æºç å®‰è£…  
    ```shell
    git clone https://github.com/openai/gym.git
    cd gym
    pip install -e .
    ```
- éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸ


```python
import gym
env = gym.make('Copy-v0')
env.reset()
env.render()

```

    Total length of input instance: 3, step: 0
    ==========================================
    Observation Tape    :   [42mD[0mBC  
    Output Tape         :   
    Targets             :   DBC  
    
    
    
    
    
    





    <ipykernel.iostream.OutStream at 0x1043684e0>



### 3.2 OpenAI æœ¯è¯­è§£é‡Š

- **è§‚æµ‹** Observation (Object)ï¼šå½“å‰ step æ‰§è¡Œåï¼Œç¯å¢ƒçš„è§‚æµ‹(ç±»å‹ä¸ºå¯¹è±¡)ã€‚ä¾‹å¦‚ï¼Œä»ç›¸æœºè·å–çš„åƒç´ ç‚¹ï¼Œæœºå™¨äººå„ä¸ªå…³èŠ‚çš„è§’åº¦æˆ–æ£‹ç›˜æ¸¸æˆå½“å‰çš„çŠ¶æ€ç­‰ï¼›

- **å¥–åŠ±** Reward (Float): æ‰§è¡Œä¸Šä¸€æ­¥åŠ¨ä½œ(action)åï¼Œæ™ºä½“(agent)è·å¾—çš„å¥–åŠ±(æµ®ç‚¹ç±»å‹)ï¼Œä¸åŒçš„ç¯å¢ƒä¸­å¥–åŠ±å€¼å˜åŒ–èŒƒå›´ä¹Ÿä¸ç›¸åŒï¼Œä½†æ˜¯å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å°±æ˜¯ä½¿å¾—æ€»å¥–åŠ±å€¼æœ€å¤§ï¼›

- **å®Œæˆ** Done (Boolen): è¡¨ç¤ºæ˜¯å¦éœ€è¦å°†ç¯å¢ƒé‡ç½® `env.reset`ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå½“ `Done` ä¸º `True` æ—¶ï¼Œå°±è¡¨æ˜å½“å‰å›åˆ(episode)æˆ–è€…è¯•éªŒ(tial)ç»“æŸã€‚ä¾‹å¦‚å½“æœºå™¨äººæ‘”å€’æˆ–è€…æ‰å‡ºå°é¢ï¼Œå°±åº”å½“ç»ˆæ­¢å½“å‰å›åˆè¿›è¡Œé‡ç½®(reset);

- **ä¿¡æ¯** Info (Dict): é’ˆå¯¹è°ƒè¯•è¿‡ç¨‹çš„è¯Šæ–­ä¿¡æ¯ã€‚åœ¨æ ‡å‡†çš„æ™ºä½“ä»¿çœŸè¯„ä¼°å½“ä¸­ä¸ä¼šä½¿ç”¨åˆ°è¿™ä¸ª infoï¼Œå…·ä½“ç”¨åˆ°çš„æ—¶å€™å†è¯´ã€‚

![](https://ws1.sinaimg.cn/large/c3a916a7gy1fui6vljemkj20gw066t93.jpg)

æ€»ç»“æ¥è¯´ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æµç¨‹ï¼Œåœ¨æ¯ä¸ªæ—¶é—´ç‚¹ä¸Šï¼Œæ™ºä½“æ‰§è¡Œ actionï¼Œç¯å¢ƒè¿”å›ä¸Šä¸€æ¬¡ action çš„è§‚æµ‹å’Œå¥–åŠ±ï¼Œç”¨å›¾è¡¨ç¤ºä¸º

## å››. ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹  Hello World

### è½¦æ†æ¨¡å‹

![](https://ws1.sinaimg.cn/large/c3a916a7gy1fuilrzjnj0j20hm0blmxu.jpg)


```python
import gym
env = gym.make('CartPole-v0')
init_state = env.reset()
print('init state = ', init_state)
for _ in range(1000):
    env.render()
    action = env.action_space.sample()
    observation, reward, done, info = env.step(action) # take a random action
    if done: 
        env.render()
        break
```

    [33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
    init state =  [ 0.03829178 -0.01427857 -0.00701367  0.00567602]


### æ¦‚å¿µè§£è¯»

- åˆ›å»ºå®ä¾‹
    - æ¯ä¸ª Gym ç¯å¢ƒéƒ½æœ‰å”¯ä¸€çš„å‘½åï¼Œå‘½åæ–¹å¼ä¸º `([A-Za-z0-9]+-)v([0-9]+)`
    - ä½¿ç”¨ `gym.make('CartPole-v0')` åˆ›å»ºç¯å¢ƒ

- é‡ç½®å‡½æ•° reset
    - ç”¨äºé‡æ–°å¼€å¯ä¸€ä¸ªæ–°çš„å›åˆ(è¯•éªŒ)
    - è¿”å›å›åˆçš„åˆå§‹çŠ¶æ€

- æ‰§è¡Œ(step)
    - æ‰§è¡Œç‰¹å®šçš„åŠ¨ä½œï¼Œè¿”å›çŠ¶æ€(state)
    - observation, reward, done, info
    
- æ¸²æŸ“(render)
    - ç”¨äºæ˜¾ç¤ºå½“å‰ç¯å¢ƒçš„çŠ¶æ€
    - ç”¨äºè°ƒè¯•å’Œå®šæ€§çš„åˆ†æä¸åŒç­–ç•¥çš„æ•ˆæœ

### ç©ºé—´(space)


```python
import gym
env = gym.make('CartPole-v0')
print(env.action_space)
#> Discrete(2)
print(env.observation_space)
#> Box(4,)

print(env.observation_space.high)
print(env.observation_space.low)
```

    [33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
    Discrete(2)
    Box(4,)
    [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]
    [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]


- è§‚æµ‹ç©ºé—´ `observation_space` æ˜¯ä¸€ä¸ª `Box` ç±»å‹ï¼Œä» [box.py](https://github.com/openai/gym/blob/master/gym/spaces/box.py) æºç å¯çŸ¥ï¼Œè¡¨ç¤ºä¸€ä¸ª `n` ç»´çš„ç›’å­ï¼Œæ‰€ä»¥åœ¨ä¸Šä¸€èŠ‚æ‰“å°å‡ºæ¥çš„ `observation` æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 4 çš„æ•°ç»„ã€‚æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½å…·æœ‰ä¸Šä¸‹ç•Œã€‚

    - Type: Box(4)

Num | Observation | Min | Max
---|---|---|---
0 | Cart Position | -2.4 | 2.4
1 | Cart Velocity | -Inf | Inf
2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;
3 | Pole Velocity At Tip | -Inf | Inf

- è¿åŠ¨ç©ºé—´ `action_space` æ˜¯ä¸€ä¸ªç¦»æ•£ `Discrete` ç±»å‹ï¼Œä» [discrete.py](https://github.com/openai/gym/blob/master/gym/spaces/discrete.py) æºç å¯çŸ¥ï¼ŒèŒƒå›´æ˜¯ä¸€ä¸ª `{0,1,...,n-1}` é•¿åº¦ä¸º `n` çš„éè´Ÿæ•´æ•°é›†åˆï¼Œåœ¨ `CartPole-v0` ä¾‹å­ä¸­ï¼ŒåŠ¨ä½œç©ºé—´è¡¨ç¤ºä¸º `{0,1}`ã€‚

    - Type: Discrete(2)

Num | Action
--- | ---
0 | Push cart to the left
1 | Push cart to the right


### å›åˆç»ˆæ­¢æ¡ä»¶(å½“æ»¡è¶³ä¸‹åˆ—æ¡ä»¶ä¹‹ä¸€æ—¶ï¼Œç»ˆæ­¢å›åˆ)

- 1. æ†çš„è§’åº¦è¶…è¿‡ $\pm12$ åº¦
- 2. ä»¥ä¸­ç‚¹ä¸ºåŸç‚¹ï¼Œå°è½¦ä½ç½®è¶…è¿‡ $\pm24$ 
- 3. å›åˆé•¿åº¦è¶…è¿‡ 200 æ¬¡


### æ±‚è§£ Cartpole æ†è½¦æ¨¡å‹

#### ç®—æ³•æ­¥éª¤

- å®šä¹‰ç­–ç•¥ Policy




#### ä»£ç æ±‚è§£


```python
import gym
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
%matplotlib inline

env = gym.make('CartPole-v0')
print('observation space:', env.observation_space)
print('action space:', env.action_space)
env.seed(0)
np.random.seed(0)

# Define policy
class Policy():
    def __init__(self, s_size=4, a_size=2):
        self.w = 1e-4*np.random.rand(s_size, a_size)  # weights for simple linear policy: state_space x action_space
        
    def forward(self, state):
        x = np.dot(state, self.w)
        return np.exp(x)/sum(np.exp(x))
    
    def act(self, state):
        probs = self.forward(state)
        #action = np.random.choice(2, p=probs) # option 1: stochastic policy
        action = np.argmax(probs)              # option 2: deterministic policy
        return action

    
policy = Policy()
    
def hill_climbing(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100, noise_scale=1e-2):
    """Implementation of hill climbing with adaptive noise scaling.
        
    Params
    ======
        n_episodes (int): maximum number of training episodes
        max_t (int): maximum number of timesteps per episode
        gamma (float): discount rate
        print_every (int): how often to print average score (over last 100 episodes)
        noise_scale (float): standard deviation of additive noise
    """
    scores_deque = deque(maxlen=100)
    scores = []
    best_R = -np.Inf
    best_w = policy.w
    for i_episode in range(1, n_episodes+1):
        rewards = []
        state = env.reset()
        for t in range(max_t):
            action = policy.act(state)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break 
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        discounts = [gamma**i for i in range(len(rewards)+1)]
        R = sum([a*b for a,b in zip(discounts, rewards)])

        if R >= best_R: # found better weights
            best_R = R
            best_w = policy.w
            noise_scale = max(1e-3, noise_scale / 2)
            policy.w += noise_scale * np.random.rand(*policy.w.shape) 
        else: # did not find better weights
            noise_scale = min(2, noise_scale * 2)
            policy.w = best_w + noise_scale * np.random.rand(*policy.w.shape)

        if i_episode % print_every == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))
        if np.mean(scores_deque)>=195.0:
            print('Environment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))
            policy.w = best_w
            break
        
    return scores
            
scores = hill_climbing()
```

    [33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
    observation space: Box(4,)
    action space: Discrete(2)
    Episode 100	Average Score: 175.24
    Environment solved in 13 episodes!	Average Score: 196.21


- ç»˜åˆ¶åˆ†æ•° reward


```python
fig = plt.figure()
ax = fig.add_subplot(111)
plt.plot(np.arange(1, len(scores)+1), scores)
plt.ylabel('Score')
plt.xlabel('Episode #')
plt.show()
```


![png](output_13_0.png)


- ç»“æœ


```python
env = gym.make('CartPole-v0')
state = env.reset()
for t in range(200):
    action = policy.act(state)
    env.render()
    state, reward, done, _ = env.step(action)
    if done:
        break 

env.close()
print('env.close()')
```

## äº”. OpenAI å¼ºåŒ–å­¦ä¹ è¿›é˜¶


```python
print('Hello reinforcement learning !\n'*4)
```

    Hello reinforcement learning !
    Hello reinforcement learning !
    Hello reinforcement learning !
    Hello reinforcement learning !
    


## å…­. æ€»ç»“ä¸æ‰©å±•

- é¡¹ç›®åœ°å€
- æ‰©å±•é˜…è¯»æ–‡çŒ® 1
- æ‰©å±•é˜…è¯»æ–‡çŒ® 2

