{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI 机器人强化学习从入门到提高 **(草稿)**\n",
    "\n",
    "> 除了试图直接去建立一个可以模拟成人大脑的程序之外， 为什么不试图建立一个可以模拟小孩大脑的程序呢?如果它接 受适当的教育，就会获得成人的大脑。 — 阿兰·图灵\n",
    "\n",
    "## 学习目的\n",
    "\n",
    "- 理论和仿真实践结合\n",
    "- 了解掌握强化学习基本原理\n",
    "- 掌握利用 Python 进行强化学习仿真\n",
    "\n",
    "## 一. 引言介绍\n",
    "\n",
    "强化学习 (Reinforcement learning) 是机器学习的一个子领域用于制定决策和运动自由度控制。强化学习主要研究在复杂未知的环境中，智体(agent)实现某个目标。强化学习最引人入胜的两个特点是\n",
    "\n",
    "- **强化学习非常通用，可以用来解决需要作出一些列决策的所有问题：**例如，训练机器人跑步和弹跳，制定商品价格和库存管理，玩 Atari 游戏和棋盘游戏等等。\n",
    "\n",
    "- **强化学习已经可以在许多复杂的环境中取得较好的实验结果：**例如 Deep RL 的 Alpha Go等\n",
    "\n",
    "[Gym](https://gym.openai.com/docs/) 是一个研究和开发强化学习相关算法的仿真平台。\n",
    "\n",
    "- 无需智体先验知识；\n",
    "- 兼容常见的数值运算库如 TensorFlow、Theano 等\n",
    "\n",
    "## 二. 强化学习的基本概念\n",
    "\n",
    "强化学习也是机器学习中的一个重要分支。强化学习和监督学习的不同在 于，强化学习问题不需要给出“正确”策略作为监督信息，只需要给出策略的(延迟)回报，并通过调整策略来取得最大化的期望回报。\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/c3a916a7gy1fui6ulth5yj20qb08r0tc.jpg)\n",
    "\n",
    "\n",
    "### 2.1 术语\n",
    "\n",
    "- 智体 (Agent)\n",
    "- 环境 (Environment)\n",
    "- 状态 (State)\n",
    "- 动作 (Action)\n",
    "- 策略 (Policy)\n",
    "- 奖励 (Reward)\n",
    "- 状态转移概率 \n",
    "\n",
    "### 2.2 马尔科夫过决策过程\n",
    "\n",
    "- 图片\n",
    "- 解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三. OpenAI 强化学习仿真环境\n",
    "\n",
    "- A standard Python API for RL environments\n",
    "- A set of tools to measure agent performance\n",
    "- An online scoreboard for comparing and benchmarking approaches\n",
    "- [ https://gym.openai.com/](https://gym.openai.com/)\n",
    "\n",
    "### 3.1 环境安装\n",
    "\n",
    "- pip 安装  \n",
    "    ```\n",
    "    pip3 install gym\n",
    "    ```\n",
    "- 源码安装  \n",
    "    ```shell\n",
    "    git clone https://github.com/openai/gym.git\n",
    "    cd gym\n",
    "    pip install -e .\n",
    "    ```\n",
    "- 验证安装是否成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of input instance: 3, step: 0\n",
      "==========================================\n",
      "Observation Tape    :   \u001b[42mD\u001b[0mBC  \n",
      "Output Tape         :   \n",
      "Targets             :   DBC  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x1043684e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Copy-v0')\n",
    "env.reset()\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 OpenAI 术语解释\n",
    "\n",
    "- **观测** Observation (Object)：当前 step 执行后，环境的观测(类型为对象)。例如，从相机获取的像素点，机器人各个关节的角度或棋盘游戏当前的状态等；\n",
    "\n",
    "- **奖励** Reward (Float): 执行上一步动作(action)后，智体(agent)获得的奖励(浮点类型)，不同的环境中奖励值变化范围也不相同，但是强化学习的目标就是使得总奖励值最大；\n",
    "\n",
    "- **完成** Done (Boolen): 表示是否需要将环境重置 `env.reset`。大多数情况下，当 `Done` 为 `True` 时，就表明当前回合(episode)或者试验(tial)结束。例如当机器人摔倒或者掉出台面，就应当终止当前回合进行重置(reset);\n",
    "\n",
    "- **信息** Info (Dict): 针对调试过程的诊断信息。在标准的智体仿真评估当中不会使用到这个 info，具体用到的时候再说。\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/c3a916a7gy1fui6vljemkj20gw066t93.jpg)\n",
    "\n",
    "总结来说，这就是一个强化学习的基本流程，在每个时间点上，智体执行 action，环境返回上一次 action 的观测和奖励，用图表示为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四. 第一个强化学习 Hello World\n",
    "\n",
    "### 车杆模型\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/c3a916a7gy1fuilrzjnj0j20hm0blmxu.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "init state =  [ 0.03829178 -0.01427857 -0.00701367  0.00567602]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "init_state = env.reset()\n",
    "print('init state = ', init_state)\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action) # take a random action\n",
    "    if done: \n",
    "        env.render()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概念解读\n",
    "\n",
    "- 创建实例\n",
    "    - 每个 Gym 环境都有唯一的命名，命名方式为 `([A-Za-z0-9]+-)v([0-9]+)`\n",
    "    - 使用 `gym.make('CartPole-v0')` 创建环境\n",
    "\n",
    "- 重置函数 reset\n",
    "    - 用于重新开启一个新的回合(试验)\n",
    "    - 返回回合的初始状态\n",
    "\n",
    "- 执行(step)\n",
    "    - 执行特定的动作，返回状态(state)\n",
    "    - observation, reward, done, info\n",
    "    \n",
    "- 渲染(render)\n",
    "    - 用于显示当前环境的状态\n",
    "    - 用于调试和定性的分析不同策略的效果\n",
    "\n",
    "### 空间(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Discrete(2)\n",
      "Box(4,)\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "#> Discrete(2)\n",
    "print(env.observation_space)\n",
    "#> Box(4,)\n",
    "\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 观测空间 `observation_space` 是一个 `Box` 类型，从 [box.py](https://github.com/openai/gym/blob/master/gym/spaces/box.py) 源码可知，表示一个 `n` 维的盒子，所以在上一节打印出来的 `observation` 是一个长度为 4 的数组。数组中的每个元素都具有上下界。\n",
    "\n",
    "    - Type: Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "- 运动空间 `action_space` 是一个离散 `Discrete` 类型，从 [discrete.py](https://github.com/openai/gym/blob/master/gym/spaces/discrete.py) 源码可知，范围是一个 `{0,1,...,n-1}` 长度为 `n` 的非负整数集合，在 `CartPole-v0` 例子中，动作空间表示为 `{0,1}`。\n",
    "\n",
    "    - Type: Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right\n",
    "\n",
    "\n",
    "### 回合终止条件(当满足下列条件之一时，终止回合)\n",
    "\n",
    "- 1. 杆的角度超过 $\\pm12$ 度\n",
    "- 2. 以中点为原点，小车位置超过 $\\pm24$ \n",
    "- 3. 回合长度超过 200 次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五. OpenAI 强化学习进阶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello reinforcement learning !\n",
      "Hello reinforcement learning !\n",
      "Hello reinforcement learning !\n",
      "Hello reinforcement learning !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Hello reinforcement learning !\\n'*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六. 总结与扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 项目地址\n",
    "- 扩展阅读文献 1\n",
    "- 扩展阅读文献 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
